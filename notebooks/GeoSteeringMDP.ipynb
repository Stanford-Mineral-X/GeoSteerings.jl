{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"../src/geosteering.jl\");\n",
    "include(\"../src/mdp.jl\");\n",
    "include(\"../src/utils.jl\");\n",
    "\n",
    "#set seed for reproducibility\n",
    "rng = MersenneTwister(1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case with tiny drifting probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the MDP model\n",
    "gs = initialize_mdp(\n",
    "    rng=rng, size=(5, 5), \n",
    "    base_amplitude=1.0, \n",
    "    base_frequency=1.0,\n",
    "    target_thickness=2.0, \n",
    "    vertical_shift=3.0,\n",
    "    drift_prob=0.001) #no drift_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one simulation step example\n",
    "p0 = initialstate(gs) # initial state distribution\n",
    "s = rand(rng, p0) # initial state s ~ p0\n",
    "\n",
    "#plot the initial state\n",
    "render(gs, (s=s,))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this 5x5 grid problem, white cells mean target zone, light gray cells mean shale zone, and darker gray cells mean harder rock. The goal is to determine the sequence of drill directions (forward, top, or bottom) from orange circle location to other end of the target zone to maximize surface coverage of the drill path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDP Model Components\n",
    "\n",
    "### State Space\n",
    "\n",
    "The state $s$ is simply the location (x, y) and binaries whether the surface area has been drilled or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ùíÆ = states(gs)\n",
    "\n",
    "#example of states\n",
    "[println(ùíÆ[i]) for i in 1:3] \n",
    "\n",
    "println(\"The size of the state space is: \", length(ùíÆ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action space\n",
    "\n",
    "The action space is relatively small, with only 4 possible actions: move forward (`RIGHT`), `UP` or `DOWN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ùíú = actions(gs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transition function\n",
    "\n",
    "The transition function in this problem is simple: whenever we take a drill action, we move toward the direction of that action with probability $1-p_\\text{drift}$, and got drifted to random location with probability $p_\\text{drift}$. However, we aren't able to go out of bounds of the system.\n",
    "\n",
    "Also, the first time we visit a new surface area (new x-coordinate), we set the Boolean visited status of that surface area to be `true`, otherwise it stays at default value `false`\n",
    "\n",
    "We can simulate a state transition as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can take random action from the action space\n",
    "\n",
    "a = rand(rng, ùíú) # random action\n",
    "p = transition(gs, s, a) # get next state distribution p(s' | s, a)\n",
    "sp = rand(rng, p) # the next state is sampled from this s ~ p\n",
    "\n",
    "println(\"From state s with location \", s.cell, \" and surrounding indicators (of being within formation/not):\")\n",
    "[println(key, \": \", value) for (key, value) in s.is_surrounding_target];\n",
    "println(\"to state sp with location \", sp.cell, \" and surroundings:\")\n",
    "[println(key, \": \", value) for (key, value) in sp.is_surrounding_target];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Had we taken an action RIGHT, we will have the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_alt = DOWN\n",
    "p_alt = transition(gs, s, a_alt) # get next state distribution p(s' | s, a)\n",
    "sp_alt = rand(rng, p_alt) # the next state is sampled from this s ~ p\n",
    "\n",
    "println(\"The new state sp with location \", sp.cell, \" and surroundings:\")\n",
    "[println(key, \": \", value) for (key, value) in sp.is_surrounding_target];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward Function\n",
    "\n",
    "The reward function `reward(s,a)` or `reward(s,a,sp)` tells the reward we get if from state `s` we take an action `a` and end up at next state `sp`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = reward(gs, s, a, sp)\n",
    "println(\"The reward for taking action \", a, \" from state \", s.cell, \" and ending up in state \", sp.cell, \" is \", r)\n",
    "\n",
    "r_alt = reward(gs, s, a_alt, sp_alt)\n",
    "println(\"The reward for taking action \", a_alt, \" from state \", s.cell, \" and ending up in state \", sp_alt.cell, \" is \", r_alt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving the model\n",
    "\n",
    "With this model, we can try a bunch of approaches to come up with a policy. For MDP, a policy is simply a mapping from state to action. You can think of it as a look up table that prescribes the optimal action to take at every state that maximize our objective.\n",
    "\n",
    "For this problem, we can formulate the objective as maximizing long-term cumulative reward of staying in the target zone:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{maximize}~ \\sum_{t=1}^T r_t \\gamma^{t-1},\n",
    "\\end{align}\n",
    "\n",
    "where $r_t$ is the reward at time $t$ and $\\gamma \\in [0, 1]$ is the discount factor that reflects how much we weigh more immediate rewards vs. future ones.\n",
    "\n",
    "We can compare two types of solvers to obtain such an optimized policy: offline solver and online solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#offline planner example\n",
    "using DiscreteValueIteration\n",
    "\n",
    "solver = ValueIterationSolver(max_iterations=100);\n",
    "@time policy = solve(solver, gs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate the policy\n",
    "hr = HistoryRecorder(max_steps=30, rng=rng)\n",
    "hist = simulate(hr, gs, policy)\n",
    "\n",
    "# render and save the plots as png images and gif animation\n",
    "plot_sim_steps = render(gs, hist);\n",
    "num_steps = length(plot_sim_steps)\n",
    "[savefig(plot_sim_steps[i], gs.size, \"../figs/SimRollout$i.png\") for i in 1:num_steps];\n",
    "create_gif_from_images(dir=\"../figs/\", gif_name=\"MDPPolicy1_NoDrifting.gif\", fps=2, num_steps=num_steps) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#online planner example\n",
    "using MCTS\n",
    "\n",
    "hr = HistoryRecorder(max_steps=30, rng=rng)\n",
    "solver = MCTSSolver(n_iterations=1000, depth=30, rng=rng)\n",
    "@time mcts_policy = solve(solver, gs);\n",
    "\n",
    "#simulate and render the policy\n",
    "@time hist = simulate(hr, gs, mcts_policy)\n",
    "\n",
    "# render and save the plots as png images and gif animation\n",
    "plot_sim_steps = render(gs, hist);\n",
    "num_steps = length(plot_sim_steps)\n",
    "[savefig(plot_sim_steps[i], gs.size, \"../figs/SimRollout$i.png\") for i in 1:num_steps];\n",
    "create_gif_from_images(dir=\"../figs/\", gif_name=\"MDPPolicy2_NoDrifting.gif\", fps=2, num_steps=num_steps) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we see that the offline solver (`Discrete Value Iteration`) runs much longer compared to the online solver (`Monte Carlo Tree Search or MCTS`), but with better policy (higher objective).  In general, offline solvers oftentimes solve for a more optimal policy (with higher objective), but runs much longer compared to the online solvers (but often subotimal or has lower objective). This is the tradeoff that we have to make in practice, especially as we deal with larger state and action spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case with Drifting Probability = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gs = initialize_mdp(\n",
    "    rng=rng, size=(5, 5), \n",
    "    base_amplitude=1.0, \n",
    "    base_frequency=1.0,\n",
    "    target_thickness=2.0, \n",
    "    vertical_shift=3.0,\n",
    "    drift_prob=0.25) #some drifting prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## offline solver with drifting\n",
    "solver = ValueIterationSolver(max_iterations=100);\n",
    "@time policy = solve(solver, gs);\n",
    "\n",
    "#simulate \n",
    "hist = simulate(hr, gs, policy)\n",
    "\n",
    "# render \n",
    "plot_sim_steps = render(gs, hist);\n",
    "num_steps = length(plot_sim_steps)\n",
    "[savefig(plot_sim_steps[i], gs.size, \"../figs/SimRollout$i.png\") for i in 1:num_steps];\n",
    "create_gif_from_images(dir=\"../figs/\", gif_name=\"MDPPolicy1_WithDrifting.gif\", fps=2, num_steps=num_steps) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## online solver with drifting\n",
    "solver = MCTSSolver(n_iterations=1000, depth=30, rng=rng)\n",
    "@time mcts_policy = solve(solver, gs);\n",
    "\n",
    "#simulate and render the policy\n",
    "@time hist = simulate(hr, gs, mcts_policy)\n",
    "\n",
    "# render \n",
    "plot_sim_steps = render(gs, hist);\n",
    "num_steps = length(plot_sim_steps)\n",
    "[savefig(plot_sim_steps[i], gs.size, \"../figs/SimRollout$i.png\") for i in 1:num_steps];\n",
    "create_gif_from_images(dir=\"../figs/\", gif_name=\"MDPPolicy2_WithDrifting.gif\", fps=2, num_steps=num_steps) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.4",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
